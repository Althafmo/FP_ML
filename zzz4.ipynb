{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2e11106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from clover import LocartSplit, RegressionScore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383c48b",
   "metadata": {},
   "source": [
    "Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e192e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 19 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   holiday     10886 non-null  int64  \n",
      " 1   workingday  10886 non-null  int64  \n",
      " 2   x           10886 non-null  float64\n",
      " 3   atemp       10886 non-null  float64\n",
      " 4   humidity    10886 non-null  int64  \n",
      " 5   windspeed   10886 non-null  float64\n",
      " 6   y           10886 non-null  int64  \n",
      " 7   season_1    10886 non-null  bool   \n",
      " 8   season_2    10886 non-null  bool   \n",
      " 9   season_3    10886 non-null  bool   \n",
      " 10  season_4    10886 non-null  bool   \n",
      " 11  weather_1   10886 non-null  bool   \n",
      " 12  weather_2   10886 non-null  bool   \n",
      " 13  weather_3   10886 non-null  bool   \n",
      " 14  weather_4   10886 non-null  bool   \n",
      " 15  hour        10886 non-null  int32  \n",
      " 16  day         10886 non-null  int32  \n",
      " 17  month       10886 non-null  int32  \n",
      " 18  year        10886 non-null  int64  \n",
      "dtypes: bool(8), float64(3), int32(3), int64(5)\n",
      "memory usage: 893.1 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"data/raw/bike/bike_train.csv\")\n",
    "\n",
    "# One-hot encode 'season' dan 'weather'\n",
    "data = pd.concat([data, \n",
    "                  pd.get_dummies(data[\"season\"], prefix=\"season\"), \n",
    "                  pd.get_dummies(data[\"weather\"], prefix=\"weather\")], axis=1)\n",
    "data.drop([\"season\", \"weather\"], axis=1, inplace=True)\n",
    "\n",
    "# Ekstrak fitur waktu dari kolom datetime\n",
    "data[\"hour\"] = pd.to_datetime(data[\"datetime\"]).dt.hour\n",
    "data[\"day\"] = pd.to_datetime(data[\"datetime\"]).dt.dayofweek\n",
    "data[\"month\"] = pd.to_datetime(data[\"datetime\"]).dt.month\n",
    "data[\"year\"] = pd.to_datetime(data[\"datetime\"]).dt.year.map({2011: 0, 2012: 1})\n",
    "\n",
    "# Hapus kolom yang tidak dipakai\n",
    "data.drop([\"datetime\", \"casual\", \"registered\"], axis=1, inplace=True)\n",
    "\n",
    "# Ganti nama kolom untuk fitur utama dan target\n",
    "data = data.rename(columns={\"temp\": \"x\", \"count\": \"y\"})\n",
    "\n",
    "# Split: train, calibration, validation\n",
    "train_df, temp_df = train_test_split(data, test_size=0.4, random_state=1500)\n",
    "cal_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=1500)\n",
    "\n",
    "# Info data\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0581446",
   "metadata": {},
   "source": [
    "Standarisasi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5466e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from clover import LocartSplit, RegressionScore\n",
    "\n",
    "# Standarisasi fitur\n",
    "scaler = StandardScaler()\n",
    "features = ['x', 'humidity', 'windspeed', 'hour', 'day', 'month', 'year']\n",
    "X_train = scaler.fit_transform(train_df[features])\n",
    "X_cal = scaler.transform(cal_df[features])\n",
    "X_val = scaler.transform(val_df[features])\n",
    "\n",
    "# Target\n",
    "y_train = train_df['y']\n",
    "y_cal = cal_df['y']\n",
    "\n",
    "# Grid search untuk Random Forest\n",
    "params = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "gcv = GridSearchCV(RandomForestRegressor(random_state=1500), params, cv=5)\n",
    "gcv.fit(X_train, y_train)\n",
    "best_rf = gcv.best_estimator_\n",
    "\n",
    "# LOCART\n",
    "alpha = 0.1\n",
    "locart = LocartSplit(RegressionScore, best_rf, alpha=alpha, is_fitted=True)\n",
    "locart.fit(X_train, y_train)\n",
    "locart.calib(X_cal, y_cal, random_seed=1500, cart_train_size=0.5)\n",
    "\n",
    "# LOFOREST\n",
    "loforest = LocartSplit(RegressionScore, best_rf, alpha=alpha, cart_type=\"RF\", is_fitted=True)\n",
    "loforest.fit(X_train, y_train)\n",
    "loforest.calib(X_cal, y_cal, random_seed=1500, cart_train_size=0.5)\n",
    "\n",
    "# Prediksi pada 500 data validasi\n",
    "x_full = val_df[features].iloc[:500]\n",
    "x_values = scaler.transform(x_full)\n",
    "y_true = val_df.loc[x_full.index, 'y'].values\n",
    "\n",
    "locart_values = locart.predict(x_values)\n",
    "loforest_values = loforest.predict(x_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b293c",
   "metadata": {},
   "source": [
    "Definisi Fungsi Evaluasi: SMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c4b5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smis(y_true, lower, upper, alpha=0.1):\n",
    "    \"\"\"Calculate Scaled Mean Interval Score (SMIS) for prediction intervals, unscaled to match paper's hundreds output.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true values.\n",
    "        lower: Array of lower bounds of prediction intervals.\n",
    "        upper: Array of upper bounds of prediction intervals.\n",
    "        alpha: Miscoverage level (e.g., 0.1 for 90% coverage).\n",
    "    \n",
    "    Returns:\n",
    "        smis: Unscaled Mean Interval Score (in hundreds).\n",
    "    \"\"\"\n",
    "    mis = (upper - lower) + (2 / alpha) * (lower - y_true) * (y_true < lower) + \\\n",
    "          (2 / alpha) * (y_true - upper) * (y_true > upper)\n",
    "    return np.mean(mis)\n",
    "\n",
    "def calculate_ccad(y_true, lower, upper, y_pred, alpha=0.1, n_bins=10):\n",
    "    \"\"\"Calculate Conditional Coverage Absolute Deviation (CCAD) by binning.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true values.\n",
    "        lower: Array of lower bounds of prediction intervals.\n",
    "        upper: Array of upper bounds of prediction intervals.\n",
    "        y_pred: Array of predicted values (e.g., median or mean) for binning.\n",
    "        alpha: Miscoverage level (e.g., 0.1 for 90% coverage).\n",
    "        n_bins: Number of bins for grouping data.\n",
    "    \n",
    "    Returns:\n",
    "        ccad: Conditional Coverage Absolute Deviation.\n",
    "    \"\"\"\n",
    "    bins = pd.qcut(y_pred, q=n_bins, duplicates='drop')\n",
    "    bin_indices = pd.Categorical(bins).codes\n",
    "    unique_bins = np.unique(bin_indices[bin_indices != -1])\n",
    "    \n",
    "    deviations = []\n",
    "    for bin_idx in unique_bins:\n",
    "        mask = bin_indices == bin_idx\n",
    "        if np.sum(mask) > 0:\n",
    "            coverage = np.mean((y_true[mask] >= lower[mask]) & (y_true[mask] <= upper[mask]))\n",
    "            deviation = np.abs(coverage - (1 - alpha))\n",
    "            deviations.append(deviation)\n",
    "    \n",
    "    return np.mean(deviations) if deviations else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d442a",
   "metadata": {},
   "source": [
    "Evaluasi Model LOCART & LOFOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04bc03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth dari subset x_full\n",
    "y_true = val_df.loc[x_full.index, 'y'].values\n",
    "\n",
    "# Prediksi ulang terhadap x_full yang sudah distandarisasi\n",
    "locart_val = locart.predict(x_values)\n",
    "loforest_val = loforest.predict(x_values)\n",
    "\n",
    "# Evaluasi LOCART\n",
    "smis_locart = calculate_smis(y_true, locart_val[:, 0], locart_val[:, 1], alpha=alpha)\n",
    "ccad_locart = calculate_ccad(y_true, locart_val[:, 0], locart_val[:, 1], y_pred_val, alpha=alpha)\n",
    "\n",
    "# Evaluasi LOFOREST\n",
    "smis_loforest = calculate_smis(y_true, loforest_val[:, 0], loforest_val[:, 1], alpha=alpha)\n",
    "ccad_loforest = calculate_ccad(y_true, loforest_val[:, 0], loforest_val[:, 1], y_pred_val, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad116541",
   "metadata": {},
   "source": [
    "Hasil model LOCART & LOFOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b6247b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SMIS and CCAD Results ===\n",
      "LOCART    -> SMIS: 191.07, CCAD: 0.0520\n",
      "LOFOREST  -> SMIS: 192.50, CCAD: 0.0460\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SMIS and CCAD Results ===\")\n",
    "print(f\"LOCART    -> SMIS: {smis_locart:.2f}, CCAD: {ccad_locart:.4f}\")\n",
    "print(f\"LOFOREST  -> SMIS: {smis_loforest:.2f}, CCAD: {ccad_loforest:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e161a",
   "metadata": {},
   "source": [
    "Perbandingan menggunakan model regresi standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08b37190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# === 1. Quantile Regression ===\n",
    "q_lower = HistGradientBoostingRegressor(loss='quantile', quantile=alpha / 2,\n",
    "                                        max_depth=2, max_iter=20, min_samples_leaf=1)\n",
    "q_upper = HistGradientBoostingRegressor(loss='quantile', quantile=1 - alpha / 2,\n",
    "                                        max_depth=2, max_iter=20, min_samples_leaf=1)\n",
    "q_median = HistGradientBoostingRegressor(loss='squared_error',\n",
    "                                         max_depth=2, max_iter=20, min_samples_leaf=1)\n",
    "\n",
    "q_lower.fit(X_train, y_train)\n",
    "q_upper.fit(X_train, y_train)\n",
    "q_median.fit(X_train, y_train)\n",
    "\n",
    "q_pred_lower = q_lower.predict(x_values)\n",
    "q_pred_upper = q_upper.predict(x_values)\n",
    "q_pred_median = q_median.predict(x_values)\n",
    "\n",
    "# === 2. Linear Regression with Normality Assumption ===\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lin = linreg.predict(x_values)\n",
    "y_train_pred_lin = linreg.predict(X_train)\n",
    "resid_std = np.std(y_train - y_train_pred_lin)\n",
    "\n",
    "z = 1.96  # Z value for 90% prediction interval\n",
    "lin_lower = y_pred_lin - z * resid_std\n",
    "lin_upper = y_pred_lin + z * resid_std\n",
    "\n",
    "# === 3. Bayesian Ridge Regression ===\n",
    "bayes = BayesianRidge()\n",
    "bayes.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bayes, std_bayes = bayes.predict(x_values, return_std=True)\n",
    "bayes_lower = y_pred_bayes - z * std_bayes\n",
    "bayes_upper = y_pred_bayes + z * std_bayes\n",
    "\n",
    "# === 4. Neural Network with MC Dropout ===\n",
    "class MCDropoutNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Buat tensor dari data\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model = MCDropoutNN(input_dim=X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  # learning rate diturunkan\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "for epoch in range(1000):  # lebih lama, tapi lebih stabil\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = loss_fn(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# MC Dropout Sampling\n",
    "model.eval()\n",
    "x_tensor = torch.tensor(x_values, dtype=torch.float32)\n",
    "mc_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        model.train()  # tetap aktifkan dropout\n",
    "        preds = model(x_tensor).squeeze().numpy()\n",
    "        mc_preds.append(preds)\n",
    "\n",
    "mc_preds = np.stack(mc_preds)\n",
    "nn_mean = mc_preds.mean(axis=0)\n",
    "nn_std = mc_preds.std(axis=0)\n",
    "nn_lower = nn_mean - z * nn_std\n",
    "nn_upper = nn_mean + z * nn_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil ground truth y dari subset data validasi yang digunakan\n",
    "y_true = val_df.loc[x_full.index, 'y'].values  \n",
    "\n",
    "# Quantile Regression\n",
    "smis_qr = calculate_smis(y_true, q_pred_lower, q_pred_upper, alpha=alpha)\n",
    "ccad_qr = calculate_ccad(y_true, q_pred_lower, q_pred_upper, q_pred_median, alpha=alpha)\n",
    "\n",
    "# Linear Regression\n",
    "smis_lin = calculate_smis(y_true, lin_lower, lin_upper, alpha=alpha)\n",
    "ccad_lin = calculate_ccad(y_true, lin_lower, lin_upper, y_pred_lin, alpha=alpha)\n",
    "\n",
    "# Bayesian Regression\n",
    "smis_bayes = calculate_smis(y_true, bayes_lower, bayes_upper, alpha=alpha)\n",
    "ccad_bayes = calculate_ccad(y_true, bayes_lower, bayes_upper, y_pred_bayes, alpha=alpha)\n",
    "\n",
    "# Neural Network MC Dropout\n",
    "smis_nn = calculate_smis(y_true, nn_lower, nn_upper, alpha=alpha)\n",
    "ccad_nn = calculate_ccad(y_true, nn_lower, nn_upper, nn_mean, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f2a8e",
   "metadata": {},
   "source": [
    "Hasil model regresi standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9baa1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model      SMIS    CCAD\n",
      "0  Quantile Regression  463.3678  0.0658\n",
      "1    Linear Regression  641.7147  0.0580\n",
      "2  Bayesian Regression  642.2072  0.0580\n",
      "3      NN + MC Dropout  673.2267  0.2440\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Quantile Regression', 'Linear Regression', 'Bayesian Regression', 'NN + MC Dropout'],\n",
    "    'SMIS': [smis_qr, smis_lin, smis_bayes, smis_nn],\n",
    "    'CCAD': [ccad_qr, ccad_lin, ccad_bayes, ccad_nn]\n",
    "})\n",
    "\n",
    "print(results.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6da2e",
   "metadata": {},
   "source": [
    "Hasil keseluruhan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa8b2a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model      SMIS    CCAD\n",
      "0               LOCART  191.0706  0.0520\n",
      "1             LOFOREST  192.4979  0.0460\n",
      "2  Quantile Regression  463.3678  0.0658\n",
      "3    Linear Regression  641.7147  0.0580\n",
      "4  Bayesian Regression  642.2072  0.0580\n",
      "5      NN + MC Dropout  673.2267  0.2440\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'LOCART', 'LOFOREST',\n",
    "        'Quantile Regression', 'Linear Regression',\n",
    "        'Bayesian Regression', 'NN + MC Dropout'\n",
    "    ],\n",
    "    'SMIS': [\n",
    "        smis_locart, smis_loforest,\n",
    "        smis_qr, smis_lin, smis_bayes, smis_nn\n",
    "    ],\n",
    "    'CCAD': [\n",
    "        ccad_locart, ccad_loforest,\n",
    "        ccad_qr, ccad_lin, ccad_bayes, ccad_nn\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(results.round(4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
